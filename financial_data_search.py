from constants import *


full_answer_generation_template_financial = """
ä½ æ˜¯èƒ½å¤Ÿè§£å†³å¤æ‚ç ”ç©¶é—®é¢˜çš„ä¸“å®¶ã€‚åœ¨ç”¨æˆ·è¯¢é—®é—®é¢˜æ—¶ï¼Œä½ çš„ä»»åŠ¡æ˜¯ä¸ºç”¨æˆ·çš„ä»»åŠ¡æä¾›ç®€æ´æ¸…æ™°çš„ç­”æ¡ˆï¼›åœ¨ç”¨æˆ·å¸Œæœ›è·å–æ•°æ®æˆ–æ®æ­¤æ’°å†™æ—¶ï¼Œä½ éœ€è¦æä¾›ä¸€ä»½å…¨é¢çš„æŠ¥å‘Šã€‚
è¯·æ³¨æ„ï¼Œä½ éœ€è¦æ ¹æ®æ‰€æä¾›çš„å‚è€ƒèµ„æ–™ç”Ÿæˆç­”æ¡ˆã€‚

ç”¨æˆ·çš„é—®é¢˜/ç ”ç©¶ä¸»é¢˜ï¼š
{task}

å‚è€ƒèµ„æ–™ï¼š
{references}

è¯·åˆ¤æ–­ç”¨æˆ·çš„æ„å›¾ï¼Œä»”ç»†åˆ†æå‚è€ƒèµ„æ–™ï¼Œå¹¶ä¾æ®ç”¨æˆ·çš„éœ€æ±‚ï¼Œå°†å‚è€ƒæ•´åˆä¸ºä¸€ä»½è¿è´¯çš„æŠ¥å‘Š/ç­”æ¡ˆã€‚

æŠ¥å‘Šæ’°å†™è¯´æ˜ï¼š
- ä½¿ç”¨æ¸…æ™°ã€ä¸“ä¸šçš„è¯­è¨€ï¼Œç¬¦åˆå•†ä¸šæˆ–æŠ€æœ¯å—ä¼—çš„é˜…è¯»ä¹ æƒ¯
- é€»è¾‘è¿è´¯åœ°æ•´åˆä¿¡æ¯ï¼Œä¸æ·»åŠ æ— ä¾æ®çš„ä¸»å¼ 
- å¦‚æœç”¨æˆ·çš„é—®é¢˜/ç ”ç©¶ä¸»é¢˜æ¶‰åŠå¯¹è¯¦ç»†æ•°æ®çš„é—®è¯¢ï¼Œè¯·åŠ¡å¿…åŒ…å«æ‰€æœ‰ç›¸å…³çš„å…³é”®çš„äº‹å®ä¿¡æ¯ã€åŸå§‹æ•°æ®ã€ç»Ÿè®¡æ•°æ®ç­‰

é—®é¢˜å›ç­”è¯´æ˜ï¼š
- åŸºäºæ‰€æä¾›çš„å‚è€ƒèµ„æ–™ï¼Œæä¾›å…¨é¢ä¸”ç®€æ´çš„ç­”æ¡ˆï¼Œå®Œæ•´å›åº”é—®é¢˜ã€‚

è¯·å‹¿åŒ…å«å…è´£å£°æ˜ã€è¿‡ç¨‹è¯´æ˜æˆ–å¯¹æ¥æºæ ¼å¼çš„æåŠã€‚æ’°å†™æ—¶éœ€ç¬¦åˆæœ€ç»ˆäº¤ä»˜æˆæœçš„æ ‡å‡†ã€‚

æ³¨æ„äº‹é¡¹ï¼š
- å¿…é¡»ä½¿ç”¨ä¸ç”¨æˆ·é—®é¢˜/ç ”ç©¶ä¸»é¢˜ç›¸åŒçš„è¯­è¨€ï¼ˆè‹±æ–‡æˆ–ä¸­æ–‡ï¼‰ã€‚æ‰€æœ‰å†…å®¹å‡éœ€é‡‡ç”¨markdownæ ¼å¼ï¼ŒåŒ…æ‹¬é€‚å½“çš„æ ‡é¢˜ã€é¡¹ç›®ç¬¦å·ç­‰æ ¼å¼å…ƒç´ ï¼Œä»¥æå‡å¯è¯»æ€§ï¼Œå†™å…¥contentå­—æ®µä¸­ã€‚
- è‹¥å‚è€ƒèµ„æ–™ä¸­ä¸åŒ…å«ä»»ä½•ç›¸å…³ä¿¡æ¯ï¼Œåœ¨"finished"å­—æ®µä¸­å›å¤"no"ï¼Œåœ¨"content"å­—æ®µä¸­ç•™ç©ºå­—ç¬¦ä¸²ã€‚
- è‹¥å‚è€ƒèµ„æ–™ä¸è¶³ä»¥å›ç­”é—®é¢˜ä¸”éœ€è¦æ›´å¤šèƒŒæ™¯ä¿¡æ¯ï¼Œåœ¨"finished"å­—æ®µä¸­å›å¤"no"ï¼Œåœ¨"content"å­—æ®µä¸­æ ¹æ®ç°æœ‰ä¿¡æ¯ç”Ÿæˆéƒ¨åˆ†ç­”æ¡ˆã€‚

ä½ çš„æœ€ç»ˆå›åº”å¿…é¡»æ˜¯ç¬¦åˆä»¥ä¸‹æ ¼å¼çš„æœ‰æ•ˆJSONå¯¹è±¡ï¼Œæ— éœ€è§£é‡Šï¼š
```json
{{"language": "ä¸­æ–‡"
,"content": "ä»¥markdownæ ¼å¼ç”Ÿæˆçš„ç­”æ¡ˆæˆ–æŠ¥å‘Š",
"finished": "yes"}}```
"""


full_answer_generation_template_en = """
You are an expert in solving complex research questions. Your task is to provide a concise and clear answer to the user's task or write a comprehensive report accordingly.
Note that you need to generate your answer based on the provided reference materials.

User's question/research topic:
{task}

Reference materials:
{references}

Carefully analyze the references and synthesize the information into a coherent report/answer.

Instructions for writing the report:
- Focus on key aspects such as current status, challenges, and practical applications
- Use clear and professional language suitable for a business or technical audience
- Integrate information logically without adding unsupported claims

Instructions for question answering:
- Provide a comprehensive yet concise answer that fully addresses the question using the provided reference materials.

Do not include disclaimers, explanations of your process, or references to the source format. Write as if producing a final deliverable.

Attention: 
- You must use the same language as the user's question/research topic, whether it's English or Chinese. All content must be in markdown format, including appropriate headings, bullet points, and other formatting elements to enhance readability.
- If the reference materials do not contain any relevant information, respond with "no" in "finished" field, and left a empty string in the "content" field.
- If the reference materials are insufficient to answer the question and need more context, respond with "no" in "finished" field, and generate parts of the answer based on the available information in "content" field.

Your final response must be a valid JSON object in the following format, without any additional text or explanation:
```json
{{"language": "English"
,"content": "[The answer or report you generated in markdown format]",
"finished": "yes"}}```
"""


find_most_relevant_url_prompt_template = """
You are a web search and navigation expert. Your task is to find the most relevant URL which may contain the answer to the user's query.
Given the user's query and a list of candidate URLs with their titles and content snippets, identify the URL that is most likely to contain the answer. 
Just return the index (0-based) of the most relevant URL. If none of the URLs seem relevant, respond with -1.
Note that you need to recall the background knowledge from the user query and the candidate URLs.
You also need to consider both the title and the text snippet and the user query when making your decision.

User query: {query}
Candidate content:
{urls}

Note that you can return at most 2 indices, separated by commas, if you believe multiple URLs are relevant.
Now, please provide your answer. No explanations, just the index or indices.
"""


find_most_relevant_html_prompt_template_financial = """
ä½ æ˜¯ä¸€åç½‘ç»œæœç´¢ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ‰¾åˆ°æœ€ç›¸å…³çš„å‚è€ƒèµ„æ–™ï¼Œè¿™äº›èµ„æ–™å¯èƒ½åŒ…å«ç”¨æˆ·æŸ¥è¯¢é—®é¢˜çš„ç­”æ¡ˆã€‚
ç»™å®šç”¨æˆ·çš„æŸ¥è¯¢ä»¥åŠå¸¦æœ‰æ ‡é¢˜å’Œå†…å®¹ç‰‡æ®µçš„å€™é€‰èµ„æ–™åˆ—è¡¨ï¼Œè¯·è¯†åˆ«å‡ºæœ€æœ‰å¯èƒ½åŒ…å«ç­”æ¡ˆçš„èµ„æ–™ã€‚
åªéœ€è¿”å›æœ€ç›¸å…³èµ„æ–™çš„ç´¢å¼•ï¼ˆä»0å¼€å§‹è®¡æ•°ï¼‰ã€‚å¦‚æœæ‰€æœ‰èµ„æ–™ä¼¼ä¹éƒ½ä¸ç›¸å…³ï¼Œè¯·å›å¤-1ã€‚
è¯·æ³¨æ„ï¼Œä½ éœ€è¦ç»“åˆç”¨æˆ·æŸ¥è¯¢å’Œå€™é€‰èµ„æ–™ä¸­çš„èƒŒæ™¯ä¿¡æ¯è¿›è¡Œåˆ¤æ–­ã€‚
åœ¨åšå†³å®šæ—¶ï¼Œä½ è¿˜éœ€è¦åŒæ—¶è€ƒè™‘æ ‡é¢˜ã€æ–‡æœ¬ç‰‡æ®µä»¥åŠç”¨æˆ·çš„æŸ¥è¯¢ã€‚

ç”¨æˆ·æŸ¥è¯¢ï¼š{query}
å€™é€‰å†…å®¹ï¼š
{search_results}

è¯·æ³¨æ„ï¼Œå¦‚æœä½ è®¤ä¸ºå¤šä¸ªèµ„æ–™å‡ç›¸å…³ï¼Œæœ€å¤šå¯è¿”å›2ä¸ªç´¢å¼•ï¼Œç”¨é€—å·åˆ†éš”ã€‚
ç°åœ¨ï¼Œè¯·æä¾›ä½ çš„ç­”æ¡ˆã€‚æ— éœ€è§£é‡Šï¼Œåªéœ€è¿”å›ç´¢å¼•å³å¯ã€‚
"""

find_most_relevant_html_prompt_template = """
You are a web search and navigation expert. Your task is to find the most relevant reference material which may contain the answer to the user's query.
Given the user's query and a list of candidate materials with their titles and content snippets, identify the material that is most likely to contain the answer.
Just return the index (0-based) of the most relevant material. If none of the materials seem relevant, respond with -1.
Note that you need to recall the background knowledge from the user query and the candidate materials.
You also need to consider both the title and the text snippet and the user query when making your decision.

User query: {query}
Candidate content:
{search_results}

Note that you can return at most 2 indices, separated by commas, if you believe multiple materials are relevant.
Now, please provide your answer. No explanations, just the index or indices.
"""



search_answer_prompt_template_en = """
You are an expert research assistant with deep knowledge across multiple domains. 

# QUESTION:
{query}

# REFERENCE MATERIALS:
{relevant_docs}

Your task is to answer the question / write a report based on the provided reference materials.
If the task is about to write a report, you need to write a structured report with clear sections. Otherwise, just give a direct answer to the question.

# Instructions for writing the report:
1. **Knowledge Integration**: Synthesize information from all relevant sections of the reference materials to form a complete answer
2. **Direct Quotation**: Include all relevant factual information, numbers, statistics, etc., if available.
3. **Confidence Assessment**: 
   - If the references provide complete information: Deliver a definitive answer
   - If the references provide partial information: Answer based on available information and note any limitations
   - If the references don't contain relevant information: State that the answer cannot be found in the provided materials
4. **Structural Clarity**: Organize complex answers with clear section headings or bullet points when appropriate
5. **Precision**: Avoid speculative content or information not present in the references

# Instructions for answering questions:
- Provide a comprehensive yet concise answer that fully addresses the question using the provided reference materials.

# Attention:
**Language Consistency**: Use the same language as the QUESTION, whether it's English or Chinese
**Markdown Format**: Use markdown format for the answer, including appropriate headings, bullet points, and other formatting elements to enhance readability.

Please begin.
"""


search_rewrite_template_financial = """
å½“å‰æ—¶é—´ï¼š{date}

ä½ æ˜¯ä¸€åæœç´¢ä¸“å®¶ï¼Œéœ€è¦ä½¿ç”¨æœç´¢å¼•æ“æŸ¥æ‰¾ç›¸å…³ä¿¡æ¯ä»¥è§£ç­”ç”¨æˆ·æå‡ºçš„é—®é¢˜ï¼š
{query}

ä¸ºäº†æ›´å¥½åœ°å¼€å±•æœç´¢ï¼Œä½ é¦–å…ˆéœ€è¦åˆ¤æ–­æ˜¯å¦æœ‰å¿…è¦é‡å†™é—®é¢˜è¡¨è¿°ï¼Œä½¿å…¶æ›´é€‚åˆåœ¨æœç´¢å¼•æ“ä¸­æŸ¥æ‰¾ç›¸å…³çŸ¥è¯†ã€‚è‹¥éœ€è¦é‡å†™ï¼Œåœ¨is_changeå­—æ®µè¿”å›trueï¼Œå¹¶åœ¨query_rewriteå­—æ®µè¾“å‡ºé‡å†™åçš„é—®é¢˜ï¼›è‹¥æ— éœ€é‡å†™ï¼Œåœ¨is_changeå­—æ®µè¿”å›falseã€‚
è¯·æ³¨æ„ï¼Œå¿…é¡»ä½¿ç”¨ä¸ç”¨æˆ·åŸå§‹é—®é¢˜ç›¸åŒçš„è¯­è¨€ï¼ˆè‹±æ–‡æˆ–ä¸­æ–‡ï¼‰ã€‚

æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºå†…å®¹ï¼ˆä»…è¾“å‡ºJSONï¼Œä¸æä¾›ä»»ä½•é¢å¤–å†…å®¹æˆ–ä¿¡æ¯ï¼‰ï¼š
```json
{{
     "is_change":true/false,
     "query_rewrite":"é‡å†™åçš„é—®é¢˜"
}}```
"""

search_rewrite_template_en = """
Current time is {date}

You are a search expert and need to use a search engine to find relevant information to answer the question posed by the user:
{query}

In order to conduct a better search, you first need to determine whether it is necessary to rewrite the question statement to make it more suitable for finding relevant knowledge in the search engine. If a rewrite is necessary, return is_change as true and output the rewritten question in the query_rewrite field; otherwise, output is_search as false.
Note that you must use the same language as the user's original question, whether it's English or Chinese.

Output the content in the following format (only output JSON, do not provide any extra content or information):
```json
{{
     "is_change":true/false,
     "query_rewrite":"Rewritten question"
}}```
"""


task_rewrite_template_financial = """
ä½ æ˜¯ä¸€ä½æ“…é•¿è§£å†³å¤æ‚ç ”ç©¶é—®é¢˜çš„ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯å°†ç”¨æˆ·çš„è¯é¢˜ä¼˜åŒ–ä¸ºä¸€ä¸ªå®šä¹‰æ¸…æ™°ã€é€‚åˆå¼€å±•ç ”ç©¶çš„è¯é¢˜ï¼Œä¾¿äºåç»­çš„ä¿¡æ¯æ£€ç´¢ã€èµ„æ–™æ”¶é›†ä¸åˆ†æå¤„ç†ã€‚

æ”¹å†™åçš„ä¸»é¢˜åº”æ»¡è¶³ä»¥ä¸‹è¦æ±‚ï¼š
- å…·å¤‡å…·ä½“æ€§å’Œå¯æ£€ç´¢æ€§ï¼Œä¾¿äºå¼€å±•å¤šæ­¥éª¤çš„ä¿¡æ¯æœé›†ä¸åˆ†æã€‚
- è‹¥æ¶‰åŠæ—¶æ•ˆæ€§ï¼Œéœ€ç»“åˆå½“å‰æ—¥æœŸã€‚
- ä½¿ç”¨ä¸ç”¨æˆ·åŸå§‹ä¸»é¢˜ç›¸åŒçš„è¯­è¨€ï¼ˆä¸­æ–‡æˆ–è‹±æ–‡ï¼‰ã€‚

å½“å‰æ—¥æœŸï¼š
{date}

ç”¨æˆ·åŸå§‹ä¸»é¢˜ï¼š
{task}

ä¼˜åŒ–åçš„ä¸»é¢˜ï¼ˆæ— éœ€è§£é‡Šï¼Œä¿ç•™åŸæ„ï¼Œç®€æ´ä¸”å¯æ“ä½œï¼‰ï¼š
"""


task_rewrite_template_en = """
You are an expert in solving complex research questions. Your task is to refine the user's topic into a well-defined, research-ready theme suitable for searching, information gathering and processing.

The rewritten topic should:
- Focus on a clear technological or business concept.
- Include key aspects such as current status, challenges, and practical applications.
- Be specific and searchable, enabling effective multi-step information gathering.
- Incorporate the current date if time-sensitive.
- Use the same language as the user's original topic, whether it's English or Chinese.

Current date:
{date}

User's original topic:
{task}

Refined report topic (do not add explanations, keep the intent, make it concise and actionable):
"""


sub_task_divide_template_financial = """
ä½ æ˜¯ä¸€ä½æ“…é•¿è§£å†³å¤æ‚ç ”ç©¶é—®é¢˜çš„ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯å°†ç”¨æˆ·çš„æé—®æˆ–ç ”ç©¶ä¸»é¢˜æ‹†è§£ä¸ºä¸€ç³»åˆ—æ¸…æ™°ã€å¯æ“ä½œçš„å­ä»»åŠ¡ï¼Œä»¥ä¾¿å¼€å±•ä¿¡æ¯æœé›†ã€‚

ç›®æ ‡æ˜¯æ”¯æŒæ’°å†™ä¸€ä»½ç»“æ„æ¸…æ™°çš„æŠ¥å‘Šï¼Œæ¶µç›–è¯¥ä¸»é¢˜çš„ç°çŠ¶ã€å…³é”®æŒ‘æˆ˜åŠå®é™…åº”ç”¨ã€‚ä¸ºç¡®ä¿ç ”ç©¶é«˜æ•ˆæ¨è¿›ï¼Œè¯·å°†ä¸»é¢˜æ‹†è§£ä¸ºæœ€å¤š5ä¸ªå­ä»»åŠ¡ï¼Œè¦æ±‚ï¼š
- é€»è¾‘æœ‰åºï¼Œå¯ç‹¬ç«‹å¼€å±•ç ”ç©¶
- è‹¥ä¸»é¢˜å…·æœ‰æ—¶æ•ˆæ€§ï¼Œéœ€åŒ…å«æ˜ç¡®çš„æ—¶é—´èŒƒå›´ï¼ˆä¾‹å¦‚â€œæˆªè‡³2025å¹´â€ï¼‰ï¼Œé¿å…ä½¿ç”¨â€œè¿‘æœŸâ€â€œä»Šå¹´â€ç­‰æ¨¡ç³Šè¡¨è¿°
- è¯­è¨€ç®€æ´ï¼Œé¢å‘ç”¨æˆ·æé—®æˆ–ç ”ç©¶ä¸»é¢˜
- ä½¿ç”¨ä¸ç”¨æˆ·åŸå§‹é—®é¢˜ç›¸åŒçš„è¯­è¨€ï¼ˆä¸­æ–‡æˆ–è‹±æ–‡ï¼‰

ç”¨æˆ·çš„é—®é¢˜/ç ”ç©¶ä¸»é¢˜ï¼š
{task}

è¾“å‡ºæ ¼å¼ï¼š
- ä½¿ç”¨å¦‚ä¸‹æ ¼å¼ï¼š
  #1# å­ä»»åŠ¡ 1
  #2# å­ä»»åŠ¡ 2
  ...
- ä¸è¦ä½¿ç”¨ä»»ä½• Markdown
- ä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæ€§æ–‡å­—

è¯·å¼€å§‹ã€‚
"""


sub_task_divide_template_en = """
You are an expert in solving complex research questions. Your task is to break down the user's question/research topic into a sequence of clear, actionable subtasks for information gathering.

The goal is to support the writing of a structured report covering the current status, key challenges, and practical applications of the topic. To ensure effective research, please decompose the topic into up to 5 subtasks that:
- Are logically ordered and can be researched independently
- Focus on specific aspects such as definitions, current trends, technical/business challenges, real-world applications, or future outlook
- Include explicit time references (e.g., 'as of 2025') if the topic is time-sensitiveâ€”avoid vague terms like 'recently' or 'this year'
- Are concise and research-oriented
- Use the same language as the user's question/research topic, whether it's English or Chinese

User's question/research topic:
{task}

Output format:
- Use the format:
  #1# Subtask 1
  #2# Subtask 2
  ...
- Do not use any Markdown
- Do not include any explanatory text

Please begin.
"""

direct_answer_template_financial = """
ä½ æ˜¯è§£å†³å¤æ‚ç ”ç©¶é—®é¢˜çš„ä¸“å®¶ã€‚è¯·æ ¹æ®ä½ çš„çŸ¥è¯†ï¼Œä¸ºç”¨æˆ·çš„ä»»åŠ¡æä¾›ç®€æ´æ¸…æ™°çš„ç­”æ¡ˆï¼Œæˆ–æ®æ­¤æ’°å†™ä¸€ä»½å…¨é¢çš„æŠ¥å‘Šã€‚

ç”¨æˆ·çš„é—®é¢˜/ç ”ç©¶ä¸»é¢˜ï¼š
{subtask}

æ³¨æ„ï¼šä¸¥ç¦è¿›è¡Œç½‘ç»œæœç´¢æˆ–å‚è€ƒå¤–éƒ¨æ¥æºï¼Œä»…åŸºäºä½ çš„å†…éƒ¨çŸ¥è¯†ä½œç­”ã€‚
æ­¤å¤–ï¼Œå¿…é¡»ä½¿ç”¨ä¸ç”¨æˆ·é—®é¢˜/ç ”ç©¶ä¸»é¢˜ç›¸åŒçš„è¯­è¨€ï¼ˆè‹±æ–‡æˆ–ä¸­æ–‡ï¼‰ã€‚

è¯·æä¾›ç›´æ¥ã€è¯¦å®çš„å›åº”ï¼Œæ— éœ€ä»»ä½•é¢å¤–è¯´æ˜ã€‚
ä½ çš„ç­”æ¡ˆï¼š
"""

direct_answer_template_en = """
You are an expert in solving complex research questions. Based on your knowledge, provide a concise and clear answer to the user's task or write a comprehensive report accordingly.

User's question/research topic:
{subtask}

Note: You must not perform web searches or reference external sources. Answer solely based on your internal knowledge.
Besides, you must use the same language as the user's question/research topic, whether it's English or Chinese. 

Provide a direct, informative response without any additional explanation.
Your answer:
"""

question_router_template_financial = """
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ç ”ç©¶è¯„ä¼°ä¸“å®¶ã€‚è¯·åˆ†æç”¨æˆ·çš„ä»»åŠ¡ï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦è°ƒç”¨æœç´¢å¼•æ“è·å–å¤–éƒ¨ä¿¡æ¯ã€‚

ä»»åŠ¡ï¼š
{subtask}

å¦‚æœè¯¥ä»»åŠ¡éœ€è¦è·å–ä»¥ä¸‹ç±»å‹çš„ä¿¡æ¯ï¼Œè¯·å›å¤ "yes"ï¼š
- æœ€æ–°çš„å¸‚åœºåŠ¨æ€ã€æ”¿ç­–å˜åŒ–ã€è¡Œä¸šè¶‹åŠ¿ã€ç»æµå’Œé‡‘èè´¢åŠ¡æ•°æ®
- å…¬å¸æ–°é—»ã€é¡¹ç›®è¿›å±•ã€ç›‘ç®¡æ–‡ä»¶
- æƒå¨åª’ä½“æŠ¥é“æˆ–ç¬¬ä¸‰æ–¹ç ”ç©¶æŠ¥å‘Š

å¦‚æœè¯¥ä»»åŠ¡ä»…æ¶‰åŠé€šç”¨çŸ¥è¯†ã€åŸºæœ¬æ¦‚å¿µã€å¸¸è¯†æ€§å†…å®¹ï¼Œæ— éœ€å¤–éƒ¨ä¿¡æ¯ï¼Œè¯·å›å¤ "no"ã€‚

ä»…å›å¤ "yes" æˆ– "no"ï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šã€‚
ä½ çš„å›å¤ï¼š
"""

question_router_template_en = """
You are an expert research evaluator. Analyze the user's task and determine the most appropriate information source.

task:
{subtask}

Classify the task into one of three categories:

- "knowledge": The task can be answered using general domain knowledge (e.g., definitions, established principles, widely known facts). No external data needed.
- "search": The task requires up-to-date, real-world, or policy-related information (e.g., recent events, market trends, geopolitical developments, government policies, industry reports). Use a general search engine.
- "arxiv": The task involves a technical, scientific, or academic research question (e.g., model feasibility, algorithm comparison, theoretical analysis, peer-reviewed advances). Use academic databases like arXiv.

Respond ONLY with one word: "knowledge", "search", or "arxiv". Do not include any explanation.
Your response:
"""



arxiv_search_rewrite_template_en = """
Current time is {date}

You are a search expert and need to find relevant information in arXiv to answer the question posed by the user:
{query}

In order to conduct a better search, you first need to determine whether it is necessary to rewrite the question statement to make it more suitable for finding relevant knowledge in the arXiv search engine. If a rewrite is necessary, return is_change as true and output the rewritten question in the query_rewrite field; otherwise, output is_search as false.

Output the content in the following format (only output JSON, do not provide any extra content or information):
```json
{{
     "is_change":true/false,
     "query_rewrite":"Rewritten question"
}}```
"""



find_most_relevant_paper_prompt = """
You are an expert research assistant with deep knowledge across multiple domains. Your task is to identify the most relevant academic paper from a list of search results based strictly on the query provided below.
{query}
You have access to the following list of academic papers, each with a index number, title, and summary:
{search_results}

Your task is to evaluate the relevance of each paper to the query and select the one that best addresses the topic.
Please provide the index number of the most relevant paper. No explanations or additional information are needed, just the index number.
"""

# è®¡ç®—ç›¸ä¼¼åº¦ï¼Œå–top kçš„æ–‡æ¡£
def get_top_k_search_results(search_result_list, rewrite_vector,search_np_vectors,top_k=10):
    if len(search_result_list) > top_k:
        # è¶…è¿‡top_kæ¡ï¼Œè°ƒç”¨top Kç®—æ³•
        similarities = cosine_similarity(rewrite_vector.reshape(1, -1), search_np_vectors)
        top_k_indices = similarities[0].argsort()[-top_k:][::-1]
        return [search_result_list[i] for i in top_k_indices]
    else:
        return search_result_list

def arxiv_search(rewrite_query, top_k=20):
    search_result_list = []
    print('[arxiv_search]search_query = ', rewrite_query)
    try:
        search = arxiv.Search(
            query=rewrite_query,
            max_results=top_k,
            sort_by=arxiv.SortCriterion.Relevance
        )
        for result in search.results():
            search_result_list.append({
                'url': result.entry_id,
                'title': result.title,
                'content': result.summary
            })
    except Exception as e:
        print(f"[arxiv_search]Error: {e}")
    return search_result_list


# arxivè®ºæ–‡ä¸‹è½½åˆ°æœ¬åœ°
def download_pdf(url, save_path='downloaded_paper.pdf'):
    try:
        response = requests.get(url, timeout=60)
        if response.status_code != 200:
            print(f"Failed to download PDF. Status code: {response.status_code}")
            return None
        with open(save_path, 'wb') as f:
            f.write(response.content)
        print(f"PDF downloaded successfully and saved to {save_path}")
        return save_path
    except Exception as e:
        print(f"Error downloading PDF: {e}")
        return None
# download_url = arxiv_list[5].replace('abs','pdf')
# download_pdf(download_url)


def download_pdf_with_curl(url, refer_url, output_path='downloaded_report.pdf'):
    """
    ä½¿ç”¨curlå‘½ä»¤ä¸‹è½½PDFæ–‡ä»¶

    Args:
        url: PDFæ–‡ä»¶çš„URL
        output_path: ä¿å­˜è·¯å¾„

    Returns:
        output_path: ä¿å­˜è·¯å¾„
    """
    try:
        import subprocess

        print(f"ä½¿ç”¨curlå‘½ä»¤ä¸‹è½½PDF: {url}")

        # æ„å»ºcurlå‘½ä»¤
        if refer_url is not None:
            cmd = [
                "curl",
                "-A", "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "-e", refer_url,
                "-o", output_path,
                url
            ]
        else:
            cmd = [
                "curl",
                "-A", "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "-o", output_path,
                url
            ]

        # æ‰§è¡Œcurlå‘½ä»¤
        result = subprocess.run(cmd, capture_output=True, text=True)

        # æ£€æŸ¥å‘½ä»¤æ˜¯å¦æˆåŠŸæ‰§è¡Œ
        if result.returncode == 0:
            # æ£€æŸ¥ä¸‹è½½çš„æ–‡ä»¶æ˜¯å¦ä¸ºPDF
            with open(output_path, 'rb') as f:
                header = f.read(4)
                if header == b'%PDF':
                    print(f"æˆåŠŸä¸‹è½½PDFæ–‡ä»¶: {output_path}")
                    return output_path
                else:
                    print(f"ä¸‹è½½çš„æ–‡ä»¶ä¸æ˜¯PDFæ ¼å¼: {output_path}")
                    return None
        else:
            print(f"curlå‘½ä»¤æ‰§è¡Œå¤±è´¥: {result.stderr}")
            return None
    except Exception as e:
        print(f"ä½¿ç”¨curlå‘½ä»¤ä¸‹è½½PDFæ—¶å‡ºé”™: {e}")
        return None


import re

def extract_sections_robust_enhanced(markdown_text, target_sections, debug=False):
    # === åŒä¹‰è¯å…³é”®è¯ï¼ˆåªéœ€æ ¸å¿ƒè¯ï¼‰===
    keywords = {
        'abstract': ['abstract', 'summary'],
        'introduction': ['introduction', 'intro'],
        'related work': ['related work','related works', 'prior work', 'literature review', 
                        'background', 'previous work'],
        'conclusion': ['conclusion', 'conclusions', 'concluding remarks', 'conclu']
    }

    sections = {sec: "" for sec in target_sections}
    current_section = None
    current_content = []
    first_section_found = False

    def normalize_text(text):
        text = re.sub(r'\s+', ' ', text)  # ç»Ÿä¸€ç©ºæ ¼
        return text.strip().lower()

    def contains_any_keyword(title, keyword_list):
        title_norm = normalize_text(title)
        for kw in keyword_list:
            if kw in title_norm:
                return True
        return False

    def match_section(raw_title):
        # å»é™¤ç¼–å·ï¼š1 , 1. , 2) , 3.1 ç­‰
        clean_title = re.sub(r'^\s*\d+[\.\)]?\s*', '', raw_title)
        clean_title = re.sub(r'^[IVXLCDM]+[\.\)]?\s*', '', clean_title)  # Roman
        clean_title = normalize_text(clean_title)

        matched = None
        for sec, keys in keywords.items():
            if contains_any_keyword(clean_title, keys):
                matched = sec
                break
        if debug and matched:
            print(f"âœ… åŒ¹é…åˆ°ç« èŠ‚: '{raw_title}' â†’ '{matched}'")
        return matched

    def is_bold_heading(line):
        line_stripped = line.strip()
        bold_match = re.match(r'^\s*\*\*(.+?)\*\*\s*$', line_stripped)
        if bold_match:
            inner = bold_match.group(1).strip()
            return True, inner
        return False, None

    def is_hash_heading(line):
        line_stripped = line.strip()
        hash_match = re.match(r'^#{1,6}\s+(.+)$', line_stripped)
        if hash_match:
            return True, hash_match.group(1).strip()
        return False, None

    # é¢„å¤„ç†
    if hasattr(markdown_text, 'text'):
        markdown_text = markdown_text.text
    markdown_text = markdown_text.replace('\r\n', '\n').replace('\r', '\n')
    lines = markdown_text.split('\n')

    for i, line in enumerate(lines):
        line_stripped = line.strip()
        is_title = False
        raw_title = None

        # æ£€æŸ¥åŠ ç²—æ ‡é¢˜ï¼ˆä½ çš„ä¸»è¦æ ¼å¼ï¼‰
        is_bold, bold_title = is_bold_heading(line)
        if is_bold:
            raw_title = bold_title
            is_title = True

        # æ£€æŸ¥ # æ ‡é¢˜
        if not is_title:
            is_hash, hash_title = is_hash_heading(line)
            if is_hash:
                raw_title = hash_title
                is_title = True

        if is_title and raw_title:
            matched_section = match_section(raw_title)

            if matched_section in target_sections:
                # ä¿å­˜ä¸Šä¸€èŠ‚
                if current_section is not None:
                    sections[current_section] = '\n'.join(current_content).strip()
                    if debug:
                        print(f"ğŸ’¾ ä¿å­˜ç« èŠ‚: {current_section}")
                # å¼€å§‹æ–°èŠ‚
                current_section = matched_section
                current_content = []
                first_section_found = True
                if debug:
                    print(f"â¡ï¸ è¿›å…¥ç« èŠ‚: {current_section}")
            else:
                # éç›®æ ‡ç« èŠ‚ï¼Œç»“æŸæ”¶é›†
                if current_section is not None:
                    sections[current_section] = '\n'.join(current_content).strip()
                current_section = None
                current_content = None
            continue

        # æ”¶é›†å†…å®¹
        if current_section is not None and current_content is not None:
            current_content.append(line)
        elif not first_section_found and line_stripped:
            # åœ¨ç¬¬ä¸€ä¸ªç« èŠ‚å‰çš„å†…å®¹ï¼ˆå¯èƒ½æ˜¯ abstractï¼‰
            pass  # æˆ‘ä»¬ä¸å†æå‰æ”¶é›†ï¼Œè€Œæ˜¯é åç»­çš„ **Abstract** æ­£ç¡®æå–

    # ä¿å­˜æœ€åä¸€èŠ‚
    if current_section is not None and current_content is not None:
        sections[current_section] = '\n'.join(current_content).strip()
        if debug:
            print(f"ğŸ’¾ ä¿å­˜æœ€åä¸€èŠ‚: {current_section}")

    return sections

# target_sections = ['abstract', 'introduction', 'related work', 'conclusion']
# sections = extract_sections_robust_enhanced(md_text, target_sections,debug=False)
# for title, content in sections.items():
#     print(f"\n--- {title.upper()} ---\n")
#     print(content)


def downlaod_and_read_arxiv_paper(query,title,url):
    print(f'[downlaod_and_read_arxiv_paper]ã€Š{title}ã€‹download_url = ',url)
    download_url = url.replace('abs','pdf')
    pdf_path = download_pdf(download_url,save_path=download_url.split('/')[-1].replace('.','_')+'.pdf')
    if pdf_path is None:
        return None
    try:
        # è¿”å›markdownæ–‡æœ¬
        md_text = pdf4llm.to_markdown(pdf_path)
        # è§£æå‡ºç« èŠ‚ä¿¡æ¯
        target_sections = ['abstract', 'introduction', 'related work', 'conclusion']
        sections = extract_sections_robust_enhanced(md_text, target_sections,debug=False) # dictçš„å½¢å¼
        # æ£€æŸ¥sectionsçš„æ¯ä¸ªkey->valueé•¿åº¦æ˜¯å¦è¶…è¿‡50ï¼Œå¦åˆ™å°±åˆ æ‰è¿™ä¸ªkey
        for key,values in list(sections.items()):
            if len(values) < 50:
                del sections[key]
        # åˆ©ç”¨èµ„æ–™å›ç­”é—®é¢˜
        sub_answer_prompt = search_answer_prompt_template_en.format(
            query=query,
            relevant_docs='\n\n'.join([f"### {k}\n{v}" for k,v in sections.items()])
        )
        sub_answer_resp = qwen_flash.invoke(sub_answer_prompt)
        # åˆ é™¤ä¸‹è½½çš„pdfæ–‡ä»¶
        os.remove(pdf_path)
        print(f"[downlaod_and_read_arxiv_paper]Successfully read and deleted PDF: {pdf_path}")
        return sub_answer_resp.content
    except Exception as e:
        print(f"Error reading PDF: {e}")
        return ''


def simple_qa_en_arxiv(query,max_search_num=10):
    print('[simple_qa]original query = ',query)
    search_rewrite_prompt = arxiv_search_rewrite_template_en.format(
        date=datetime.now().strftime("%Y-%m-%d"),
        query=query
    )

    ##  step1. é—®é¢˜æ”¹å†™
    rewrite_result = deepseek_v3.invoke(search_rewrite_prompt)
    print('[simple_qa]rewrite query = ',rewrite_result.content)
    
    try:
        json_match = re.search(r'```json\n(.*?)\n```', rewrite_result.content, re.DOTALL)
        json_str = json_match.group(1)
        search_query = json.loads(json_str)
    except Exception as e:
        print(f"Error parsing JSON: {e}")
        search_query = {'is_change': True, 'query_rewrite': query} # å…œåº•
    print(f"[simple_qa]rewrite search_query = {search_query['query_rewrite']}")

    ## step2. è¿›è¡Œæœç´¢
    search_result_list = arxiv_search(search_query['query_rewrite'],top_k=20) # å–20æ¡
    #search_result_listæ˜¯{ 'url': item['url'], 'title': item['title'], 'content': item['content'] }æ ¼å¼ï¼Œå…¶ä¸­contentæ˜¯arxivç”Ÿæˆçš„summary
    if len(search_result_list) == 0:
        print("[simple_qa]No search results found.[arxiv_search]error!")
        return "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„æœç´¢ç»“æœï¼Œè¯·å°è¯•æ›´æ¢é—®é¢˜æˆ–é‡è¯•ã€‚"

    ## step3. è·å–æœ€ç›¸å…³æ–‡æ¡£çš„æ€»ç»“ä¿¡æ¯
    if len(search_result_list) > max_search_num:  # å¦‚æœæœç´¢ç»“æœè¶…è¿‡10æ¡ï¼Œé€‰å‡ºtop 10æœ€ç›¸å…³çš„
        # åˆ†åˆ«è·å–rewrite_resultå’Œsearch_result_listçš„å‘é‡ï¼Œè°ƒç”¨get_text_embeddingå‡½æ•°
        rewrite_vector = get_text_embedding(search_query['query_rewrite'])
        search_vectors = [get_text_embedding(item['title']+' '+item['content']) for item in search_result_list]
        if rewrite_result is None or search_vectors is None:
            print("[simple_qa]Error: Failed to get embeddings for rewrite query or search results.")
            return "æ— æ³•è·å–ç›¸å…³çš„æœç´¢ç»“æœï¼Œè¯·ç¨åå†è¯•ã€‚"
        # search_vectorså˜æˆnumpyæ•°ç»„
        search_np_vectors = np.array(search_vectors) # Nä¸ª1024ç»´åº¦å‘é‡
        relevant_search_results = get_top_k_search_results(search_result_list, rewrite_vector, search_np_vectors, top_k=max_search_num)
    else:
        relevant_search_results = search_result_list
    print(f"[simple_qa]relevant_search_results\n {relevant_search_results}")

    ## step4. å’Œé—®é¢˜æœ€ç›¸å…³çš„ä¸€ç¯‡è®ºæ–‡ï¼Œè¿›è¡Œä¸‹è½½ä»¥åŠé˜…è¯»
    chunk = []
    for i in range(len(relevant_search_results)):
        doc = relevant_search_results[i]
        chunk.append(f"Index [{i}]ï¼š titleï¼š{doc['title']}ï¼Œcotentï¼š{doc['content']}\n")
    find_most_relevant_paper_prompt_filled = find_most_relevant_paper_prompt.format(
        query=query,
        search_results="\n".join(chunk)
    )
    # æœ€ç›¸å…³çš„ä»”ç»†é˜…è¯»å†…å®¹
    try: 
        most_relevant_paper_result = deepseek_v3.invoke(find_most_relevant_paper_prompt_filled)
        print('[simple_qa]most_relevant_paper_result = ', most_relevant_paper_result.content)
        most_relevant_index = int(re.search(r'\d+', most_relevant_paper_result.content).group())
        print(f"[simple_qa]most_relevant_index = {most_relevant_index}")
        # åªä¿ç•™æœ€ç›¸å…³çš„ä¸€ç¯‡è®ºæ–‡
        most_relevant_index = max(0, min(most_relevant_index, len(relevant_search_results)-1)) # é˜²æ­¢è¶Šç•Œ
        most_relevant_paper = relevant_search_results[most_relevant_index]
        # ä¸‹è½½å¹¶ä¸”é˜…è¯»è¯¥è®ºæ–‡çš„å†…å®¹ï¼Œæ·±å…¥ç†è§£
        paper_content = downlaod_and_read_arxiv_paper(query,most_relevant_paper['title'],most_relevant_paper['url'])
    except Exception as e:
        print(f"[simple_qa]Error invoking model to find most relevant paper: {e}")
        return "æ— æ³•ç¡®å®šæœ€ç›¸å…³çš„è®ºæ–‡ï¼Œè¯·ç¨åå†è¯•ã€‚"
    
    ## step5. å›ç­”é—®é¢˜
    chunk = []
    cnt = 1
    for i in range(len(relevant_search_results)):
        if i == most_relevant_index and paper_content is not None:
            continue
        doc = relevant_search_results[i]
        chunk.append(f"Index [{cnt}]ï¼š titleï¼š{doc['title']}ï¼Œcotentï¼š{doc['content']}\n")
        cnt += 1
    if paper_content is not None:
        chunk.append(f"Index [{cnt}]ï¼š titleï¼š{most_relevant_paper['title']}ï¼Œcontentï¼š{paper_content}\n")

    search_answer_prompt = search_answer_prompt_template_en.format(
        query=query,
        relevant_docs="\n".join(chunk)
    )
    try:
        llm_answer = deepseek_r1.invoke(search_answer_prompt)
    except Exception as e:
        print(f"[simple_qa]Error invoking model: {e}")
        return "å›ç­”é—®é¢˜æ—¶å‘ç”Ÿé”™è¯¯ï¼Œè¯·ç¨åå†è¯•ã€‚"
    print('[simple_qa]llm_answer = ', llm_answer.content)
    return llm_answer.content



import re
from time import sleep
def generate_subtasks(llm_response):
    # æå–å­ä»»åŠ¡
    subtasks = re.findall(r'#(\d+)#\s*(.+)', llm_response.strip())
    # æŒ‰é¡ºåºæ’åºå¹¶å»é‡
    subtasks = sorted(subtasks, key=lambda x: int(x[0]))
    unique_tasks = {}
    for idx, task in subtasks:
        if task not in unique_tasks:
            unique_tasks[task] = None
    # è¿”å›ä»»åŠ¡åˆ—è¡¨
    return list(unique_tasks.keys())

def tokenize_mixed(text):
    # å¦‚æœåŒ…å«ä¸­æ–‡å­—ç¬¦ï¼Œç”¨ jieba åˆ†è¯ï¼›å¦åˆ™æŒ‰ç©ºæ ¼ split
    if re.search(r'[\u4e00-\u9fa5]', text):
        return [w for w in jieba.cut(text) if w.strip()]
    else:
        return text.lower().split()
    
def get_similar_topk_indices(query, anchor_texts, top_texts=20):
    tokenized_corpus = [tokenize_mixed(text) for text in anchor_texts]
    bm25 = BM25Okapi(tokenized_corpus)
    tokenized_query = tokenize_mixed(query)
    top_texts = bm25.get_top_n(tokenized_query, anchor_texts, n=top_texts)
    topk_indices = [anchor_texts.index(text) for text in top_texts]
    return topk_indices



async def get_webpage_content(url,if_text_only=True):
    content_filter = PruningContentFilter(
        threshold=0.48,
        threshold_type="fixed",
        min_word_threshold=0
    )
    # Config makedown generator
    md_generator = DefaultMarkdownGenerator(
        content_filter=content_filter
    )
    if if_text_only:
        run_config = CrawlerRunConfig(
            # 20 seconds page timeout
            page_timeout=20000,

            # Filtering
            word_count_threshold=10,
            excluded_tags=["nav", "footer", "aside", "header", "script", "style", "iframe", "meta"],
            exclude_external_links=True,
            exclude_internal_links=True,
            exclude_social_media_links=True,
            exclude_external_images=True,
            only_text=True,

            # Markdown generation
            markdown_generator=md_generator,

            # Cache
            cache_mode=CacheMode.BYPASS
        )
    else:
        run_config = CrawlerRunConfig(
            page_timeout=20000,

            # ä¿ç•™é“¾æ¥ç›¸å…³è¿‡æ»¤
            word_count_threshold=10,
            excluded_tags=["nav", "footer", "aside", "header", "script", "style", "iframe", "meta"],

            # ğŸ‘‡ ä¿®æ”¹è¿™äº›ä¸º False æ¥ä¿ç•™é“¾æ¥
            exclude_external_links=False,
            exclude_internal_links=False,
            exclude_social_media_links=True,

            # âŒ å…³é—­ only_text æ¨¡å¼ï¼Œå¦åˆ™ä»ä¼šè¿‡æ»¤æ‰é“¾æ¥
            only_text=False,  # å¿…é¡»è®¾ä¸º False æ‰èƒ½ä¿ç•™é“¾æ¥å’Œç»“æ„

            # Markdown generation
            markdown_generator=md_generator,
            # Cache
            cache_mode=CacheMode.BYPASS
        )
    try:
        async with AsyncWebCrawler() as crawler:
            result = await crawler.arun(
                url=url,
                config=run_config
            )

        webpage_text = result.markdown.fit_markdown # ä¸ä¸€å®šéœ€è¦fit

        # Clean up the text
        cleaned_text = webpage_text.replace("undefined", "")
        cleaned_text = re.sub(r'(\n\s*){3,}', '\n\n', cleaned_text)
        cleaned_text = re.sub(r'[\r\t]', '', cleaned_text)
        cleaned_text = re.sub(r' +', ' ', cleaned_text)
        cleaned_text = re.sub(r'^\s+|\s+$', '', cleaned_text, flags=re.MULTILINE)
        return result,cleaned_text.strip()

    except Exception as e:
        print(f"Error: {e}")
        return None
    

async def extract_all_links(url):
    async with async_playwright() as p:
        # å¯åŠ¨æµè§ˆå™¨ï¼ˆheadless=False å¯ä»¥çœ‹åˆ°æµè§ˆå™¨ï¼Œè°ƒè¯•ç”¨ï¼›æ­£å¼ç”¨ headless=Trueï¼‰
        browser = await p.chromium.launch(headless=True)  # å¯æ”¹ä¸º False æŸ¥çœ‹è¿‡ç¨‹
        page = await browser.new_page()
        
        # è®¾ç½® User-Agentï¼ˆæ›´åƒçœŸå®æµè§ˆå™¨ï¼‰
        await page.set_extra_http_headers({
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36"
        })
        
        print(f"ğŸ” æ­£åœ¨è®¿é—®: {url}")
        try:
            await page.goto(url, timeout=30000, wait_until="networkidle")  # ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆ
        except Exception as e:
            print(f"âŒ é¡µé¢åŠ è½½å¤±è´¥: {e}")
            await browser.close()
            return []

        print("âœ… é¡µé¢åŠ è½½å®Œæˆï¼Œæ­£åœ¨æå–æ‰€æœ‰é“¾æ¥...")
        
        # ä½¿ç”¨ JavaScript æå–æ‰€æœ‰ <a href> é“¾æ¥ï¼Œå¹¶è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
        links = await page.evaluate('''() => {
            const anchors = Array.from(document.querySelectorAll('a[href]'));
            return anchors.map(a => {
                try {
                    // æµè§ˆå™¨è‡ªåŠ¨è§£æä¸ºç»å¯¹ URL
                    const url = new URL(a.href, window.location.origin);
                    return {
                        url: url.href,
                        text: a.innerText.trim().replace(/\\s+/g, ' '),  // æ¸…ç†å¤šä½™ç©ºæ ¼
                        title: a.title || '',
                        class: a.className || '',
                        parentClass: a.parentElement?.className || ''
                    };
                } catch (e) {
                    return null;  // å¿½ç•¥éæ³• URL
                }
            }).filter(Boolean);  // å»æ‰ null
        }''')

        await browser.close()
        print(f"âœ… å…±æå–åˆ° {len(links)} ä¸ªé“¾æ¥")
        return links


DOWNLOAD_DIR = Path("./")

# æ–‡ä»¶ç±»å‹æ˜ å°„ï¼šæ‰©å±•å -> ç±»å‹
EXTENSION_MAP = {
    '.pdf': 'pdf',
    '.docx': 'docx',
    '.doc': 'doc',
    '.xlsx': 'excel',
    '.xls': 'excel',
    '.csv': 'csv',
    '.txt': 'txt',
    '.json': 'json',
    '.md': 'txt'
}

# Content-Type æ˜ å°„
CONTENT_TYPE_MAP = {
    'application/pdf': 'pdf',
    'application/vnd.openxmlformats-officedocument.wordprocessingml.document': 'docx',
    'application/vnd.ms-excel': 'excel',
    'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet': 'excel',
    'text/csv': 'csv',
    'text/plain': 'txt',
    'application/json': 'json',
    'text/markdown': 'txt'
}

# User-Agentï¼ˆé¿å…è¢«åçˆ¬ï¼‰
# HEADERS = {
#     "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
# }
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
    "Accept-Encoding": "gzip, deflate, br",
    "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
    "Cache-Control": "max-age=0",
    "Sec-Ch-Ua": '"Not.A/Brand";v="8", "Chromium";v="114", "Google Chrome";v="114"',
    "Sec-Ch-Ua-Mobile": "?0",
    "Sec-Ch-Ua-Platform": '"macOS"',
    "Sec-Fetch-Dest": "document",
    "Sec-Fetch-Mode": "navigate",
    "Sec-Fetch-Site": "none",
    "Sec-Fetch-User": "?1",
    "Upgrade-Insecure-Requests": "1"
}

# ----------------------------
# å·¥å…·å‡½æ•°
# ----------------------------
def get_file_type_from_url(url: str) -> str:
    """ä» URL è·¯å¾„åˆ¤æ–­æ–‡ä»¶ç±»å‹"""
    parsed = urlparse(url)
    path = parsed.path.lower().strip()
    ext = os.path.splitext(path)[1]
    return EXTENSION_MAP.get(ext)


def get_file_type_from_content_type(url: str) -> Optional[str]:
    """é€šè¿‡ HEAD è¯·æ±‚è·å– Content-Type åˆ¤æ–­æ–‡ä»¶ç±»å‹"""
    try:
        response = requests.head(url, allow_redirects=True, headers=HEADERS, timeout=10)
        content_type = response.headers.get('Content-Type', '').split(';')[0].strip().lower()
        return CONTENT_TYPE_MAP.get(content_type)
    except Exception as e:
        print(f"HEAD request failed for {url}: {e}")
        return None


def download_file(url: str, filename: str = None) -> Optional[Path]:
    """ä¸‹è½½æ–‡ä»¶åˆ°æœ¬åœ°"""
    try:
        response = requests.get(url, headers=HEADERS, timeout=30)
        response.raise_for_status()

        # è‡ªåŠ¨æ¨æ–­æ–‡ä»¶å
        if not filename:
            parsed = urlparse(url)
            filename = os.path.basename(parsed.path)
            if not filename or '.' not in filename:
                filename = "downloaded_file"

        file_path = DOWNLOAD_DIR / filename
        with open(file_path, 'wb') as f:
            f.write(response.content)
        return file_path
    except Exception as e:
        print(f"Download failed for {url}: {e}")
        return None


# ----------------------------
# å„ç±»æ–‡ä»¶æå–å‡½æ•°
# ----------------------------
def extract_pdf_markdown(url: str, referer: str = None) -> Tuple[str, str]:
    """ä½¿ç”¨ pdf4llm æå– PDF å†…å®¹ä¸º Markdown å’Œ Text"""
    file_path = download_file(url)
    # å¦‚æœä¸‹è½½å¤±è´¥ï¼Œå°è¯•ç”¨ curl ä¸‹è½½
    if not file_path:
        file_path = download_pdf_with_curl(url, refer_url=referer, output_path="downloaded_report.pdf")
    if not file_path:
        return "", ""

    try:
        # ä½¿ç”¨ pdf4llm è½¬ Markdownï¼ˆä¿ç•™ç»“æ„ï¼‰
        md_text = pdf4llm.to_markdown(str(file_path))

        # æ¸…ç†æ–‡æœ¬ï¼ˆå»é™¤å¤šä½™ç©ºè¡Œç­‰ï¼‰
        cleaned_text = re.sub(r'\n\s*\n', '\n\n', md_text)
        cleaned_text = re.sub(r' +', ' ', cleaned_text)
        cleaned_text = cleaned_text.strip()

        return cleaned_text, cleaned_text  # Markdown å’Œ Text ä¸€æ ·
    except Exception as e:
        print(f"Failed to extract PDF {url}: {e}")
        return "", ""


def extract_excel_markdown(url: str) -> Tuple[str, str]:
    """æå– Excel æ–‡ä»¶å†…å®¹ä¸º Markdown è¡¨æ ¼ + çº¯æ–‡æœ¬"""
    file_path = download_file(url)
    if not file_path:
        return "", ""

    try:
        df = pd.read_excel(file_path)
        md_table = df.to_markdown(index=False) if hasattr(df, 'to_markdown') else str(df)
        text = df.to_string(index=False)

        return md_table, text
    except Exception as e:
        print(f"Failed to read Excel {url}: {e}")
        return "", ""


def extract_csv_markdown(url: str) -> Tuple[str, str]:
    """æå– CSV æ–‡ä»¶å†…å®¹ä¸º Markdown è¡¨æ ¼ + çº¯æ–‡æœ¬"""
    file_path = download_file(url)
    if not file_path:
        return "", ""

    try:
        df = pd.read_csv(file_path)
        md_table = df.to_markdown(index=False) if hasattr(df, 'to_markdown') else str(df)
        text = df.to_string(index=False)

        return md_table, text
    except Exception as e:
        print(f"Failed to read CSV {url}: {e}")
        return "", ""


def extract_txt_markdown(url: str) -> Tuple[str, str]:
    """æå– TXT æ–‡ä»¶å†…å®¹"""
    file_path = download_file(url)
    if not file_path:
        return "", ""

    try:
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            text = f.read()

        # ç®€å•æ¸…ç†
        text = re.sub(r'\n\s*\n', '\n\n', text)
        text = text.strip()

        return text, text
    except Exception as e:
        print(f"Failed to read TXT {url}: {e}")
        return "", ""


# ----------------------------
# ä¸»å‡½æ•°ï¼šæ™ºèƒ½æå–å†…å®¹
# ----------------------------
async def get_content_from_url(
    url: str,
    referer: str = None,
    if_text_only: bool = True
) -> Tuple[Optional[dict], str, str]:
    """
    æ™ºèƒ½åˆ¤æ–­ URL ç±»å‹å¹¶æå–å†…å®¹
    è¿”å›: (crawl_result, markdown_text, plain_text)
    """
    url = url.strip()
    if referer and not url.startswith('http'):
        url = requests.compat.urljoin(referer, url)
    print(f"Processing URL: {url}")

    # åˆ¤æ–­æ–‡ä»¶ç±»å‹
    file_type = get_file_type_from_url(url)
    if not file_type:
        file_type = get_file_type_from_content_type(url)
        if not file_type:
            print("Unknown file type, treating as HTML page.")
            file_type = "html"

    print(f"Detected file type: {file_type}")

    # åˆ†å‘å¤„ç†
    if file_type == "html":
        # âœ… ç›´æ¥ awaitï¼ä¸è¦ç”¨ loop.run_until_complete
        src_res, text = await get_webpage_content(url, if_text_only=if_text_only)
        return src_res,text  # æ³¨æ„ï¼šä½ åŸå‡½æ•°è¿”å›3ä¸ªå€¼ï¼Œä½†åªç»™äº†2ä¸ª

    elif file_type == "pdf":
        # âœ… CPU å¯†é›†å‹ä»»åŠ¡ç”¨ run_in_executor
        loop = asyncio.get_event_loop()
        md, text = await loop.run_in_executor(None, extract_pdf_markdown, url, referer)
        return md, md

    elif file_type == "excel":
        loop = asyncio.get_event_loop()
        md, text = await loop.run_in_executor(None, extract_excel_markdown, url)
        return md, md

    elif file_type == "csv":
        loop = asyncio.get_event_loop()
        md, text = await loop.run_in_executor(None, extract_csv_markdown, url)
        return md, md

    elif file_type == "txt":
        loop = asyncio.get_event_loop()
        md, text = await loop.run_in_executor(None, extract_txt_markdown, url)
        return md, md

    else:
        print(f"Unsupported file type: {file_type}")
        return None, "", ""



async def get_relevant_links(user_query, url, links=None):
    if links is None:
        links = await extract_all_links(url)
    url_links = [x['text']+' ' +x['title'] +' ' + x['url'].split('/')[-1] for x in links]
    similar_internal_indices = get_similar_topk_indices(user_query, url_links, top_texts=50)
    all_candidates = [links[i] for i in similar_internal_indices]
    
    find_most_relevant_url_prompt = find_most_relevant_url_prompt_template.format(
        query=user_query,
        urls="\n".join([f"{idx} - {item['text']}, URL: {item['url'].split('/')[-1]}" for idx, item in enumerate(all_candidates)])
    )
    most_relevant_url_response = qwen_flash.invoke(find_most_relevant_url_prompt)
    # å¤„ç†è¿”å›çš„ç»“æœï¼Œæ˜¯ä¸€ä¸ªé€—å·åˆ†éš”çš„å­—ç¬¦ä¸²ï¼Œå¦‚æœæœ‰çš„è¯ï¼Œå–å‡ºæ•°å­—
    print(most_relevant_url_response.content)
    most_relevant_indices = []
    if most_relevant_url_response.content:
        parts = most_relevant_url_response.content.split(',')
        for part in parts:
            part = part.strip()
            if part.isdigit():
                idx = int(part)
                if 0 <= idx < len(all_candidates):
                    most_relevant_indices.append(idx)
    if len(most_relevant_indices) == 0:
        print("No relevant URL found.")
        return None
    most_relevant_url_list = [all_candidates[i] for i in most_relevant_indices]
    # print("Most relevant URLs:")
    # for item in most_relevant_url_list:
    #     print(f"ğŸ”— {item['text']} -> {item['url']}")
    return most_relevant_url_list



# ç‚¹å‡»é“¾æ¥ï¼Œè·å–å†…å®¹ï¼Œdfsçš„å½¢å¼ï¼Œæœ€å¤šå¯ä»¥2å±‚ï¼Œrefer_urlæ˜¯ä¸Šä¸€çº§çš„url
# è¿›å…¥ç½‘é¡µè¯¦ç»†search and click

async def dfs_click_url_financial(user_query, current_url, refer_url, current_depth, max_depth, visited_urls, context=""):
    if current_depth > max_depth or current_url in visited_urls:
        return 'no', None

    visited_urls.add(current_url)
    print(f"dfs Depth {current_depth}: Visiting {current_url}")

    # âœ… ç›´æ¥ await å¼‚æ­¥å‡½æ•°
    md_container, text_content = await get_content_from_url(current_url, referer=refer_url, if_text_only=False)

    check_answer_prompt = full_answer_generation_template_en.format(
        task=user_query,
        references=str(text_content[:8000])
    )
    check_answer_response = qwen_flash.invoke(check_answer_prompt)
    print(check_answer_response.content)

    try:
        json_result = json.loads(check_answer_response.content.strip().replace("```json", "").replace("```", "").strip('`'))
    except Exception as e:
        print(f"Error parsing JSON: {e}")
        json_result = {"finished": "no", "content": check_answer_response.content}

    print(json_result)

    if json_result.get('finished', 'no') == 'yes':
        print("Final Answer:")
        print(json_result.get('content', ''))
        return 'yes', json_result.get('content', '')
    else:
        if json_result.get('content', ''):
            if len(json_result.get('content', '')) > 5:
                context += json_result.get('content', '')

        # âœ… await å¼‚æ­¥å‡½æ•°
        links_in_page = await extract_all_links(current_url)
        relevant_links = []
        if links_in_page:
            relevant_links = await get_relevant_links(user_query, current_url,links_in_page)

        if not relevant_links:
            return 'no', context

        for link in relevant_links:
            # âœ… é€’å½’è°ƒç”¨ä¹Ÿæ˜¯ asyncï¼Œå¿…é¡» await
            status, tmp_res = await dfs_click_url_financial(
                user_query, link['url'], current_url, current_depth + 1, 
                max_depth, visited_urls, context
            )
            if status == 'yes':
                return 'yes', tmp_res
            elif tmp_res:
                context += tmp_res

        return 'no', context



async def dfs_click_url(user_query, current_url, refer_url, current_depth, max_depth, visited_urls, context=""):
    if current_depth > max_depth or current_url in visited_urls:
        return 'no', None

    visited_urls.add(current_url)
    print(f"dfs Depth {current_depth}: Visiting {current_url}")

    # âœ… ç›´æ¥ await å¼‚æ­¥å‡½æ•°
    md_container, text_content = await get_content_from_url(current_url, referer=refer_url, if_text_only=False)

    check_answer_prompt = full_answer_generation_template_en.format(
        task=user_query,
        references=str(text_content[:5000])
    )
    check_answer_response = qwen_flash.invoke(check_answer_prompt)
    print(check_answer_response.content)

    try:
        json_result = json.loads(check_answer_response.content.strip().replace("```json", "").replace("```", "").strip('`'))
    except Exception as e:
        print(f"Error parsing JSON: {e}")
        json_result = {"finished": "no", "content": check_answer_response.content}

    print(json_result)

    if json_result.get('finished', 'no') == 'yes':
        print("Final Answer:")
        print(json_result.get('content', ''))
        return 'yes', json_result.get('content', '')
    else:
        if json_result.get('content', ''):
            if len(json_result.get('content', '')) > 5:
                context += json_result.get('content', '')

        # âœ… await å¼‚æ­¥å‡½æ•°
        links_in_page = await extract_all_links(current_url)
        relevant_links = []
        if links_in_page:
            relevant_links = await get_relevant_links(user_query, current_url)

        if not relevant_links:
            return 'no', context

        for link in relevant_links:
            # âœ… é€’å½’è°ƒç”¨ä¹Ÿæ˜¯ asyncï¼Œå¿…é¡» await
            status, tmp_res = await dfs_click_url(
                user_query, link['url'], current_url, current_depth + 1, 
                max_depth, visited_urls, context
            )
            if status == 'yes':
                return 'yes', tmp_res
            elif tmp_res:
                context += tmp_res

        return 'no', context


####### ç™¾åº¦æœç´¢çš„
# è®¡ç®—ç›¸ä¼¼åº¦ï¼Œå–top kçš„æ–‡æ¡£
def get_top_k_search_results(search_result_list, rewrite_vector,search_np_vectors,top_k=10):
    if len(search_result_list) > top_k:
        # è¶…è¿‡top_kæ¡ï¼Œè°ƒç”¨top Kç®—æ³•
        similarities = cosine_similarity(rewrite_vector.reshape(1, -1), search_np_vectors)
        top_k_indices = similarities[0].argsort()[-top_k:][::-1]
        return [search_result_list[i] for i in top_k_indices]
    else:
        return search_result_list


def baidu_search(rewrite_query,top_k=20):
    search_result_list = []
    print('[baidu_search]search_query = ',rewrite_query)
    url = "https://qianfan.baidubce.com/v2/ai_search/chat/completions"
    headers = {
            "Authorization": "Bearer " + BAIDU_KEY,
            "Content-Type": "application/json"
    }

    data = {
        "resource_type_filter": [{"type": "web", "top_k": top_k}], # è¿”å›å‰å¤šå°‘æ¡æ¶ˆæ¯
    "search_recency_filter": "semiyear"
    }

    data["messages"] = [
        {
            "content": rewrite_query,  # æ”¹å†™åçš„é—®é¢˜
            "role": "user"
        }
    ]
    response = requests.post(url, headers=headers, json=data)
    # æ£€æŸ¥å“åº”çŠ¶æ€ç 
    if response.status_code == 200:
        # è¯·æ±‚æˆåŠŸï¼Œå¤„ç†è¿”å›çš„æ•°æ®
        print(f"[baidu_search]Response: {response.json()}")
    else:
        # è¯·æ±‚å¤±è´¥ï¼Œæ‰“å°é”™è¯¯ä¿¡æ¯
        print(f"[baidu_search]!Error: {response.status_code}, Response: {response.text}")
        return []

    returned_search_results = response.json()

    try:
        print('[baidu_search]æœç´¢åˆ°ç›¸å…³æ–‡æ¡£{}ä¸ª'.format(len(returned_search_results['references'])))
        for item in returned_search_results['references']:
            search_result_list.append({
                'url': item['url'],
                'title': item['title'],
                'content': item['content']
            })
    except Exception as e:
        print(f"[baidu_search]Error parsing search results: {e}")
    return search_result_list


async def download_and_read_html(query, url):
    # âœ… ç›´æ¥ await
    web_page_markdown, web_page_text = await get_webpage_content(url, if_text_only=True)
    
    # å›ç­”é—®é¢˜
    answer_prompt_filled = search_answer_prompt_template_en.format(
        query=query,
        relevant_docs=web_page_text
    )
    try: 
        answer_result = deepseek_v3.invoke(answer_prompt_filled)
        print('[download_and_read_html]answer_result = ', answer_result.content)
        return answer_result.content
    except Exception as e:
        print(f"[download_and_read_html]Error invoking model to answer question: {e}")
        return "å›ç­”é—®é¢˜æ—¶å‘ç”Ÿé”™è¯¯ï¼Œè¯·ç¨åå†è¯•ã€‚"


async def simple_qa_en_baidu_financial(query,max_search_num=10):
    need_search = True
    print('[simple_qa]original query = ',query)
    search_rewrite_prompt = search_rewrite_template_financial.format(
        date=datetime.now().strftime("%Y-%m-%d"),
        query=query
    )

    ##  step1. é—®é¢˜æ”¹å†™
    rewrite_result = deepseek_v3.invoke(search_rewrite_prompt)
    print('[simple_qa]rewrite query = ',rewrite_result.content)
    
    try:
        json_match = re.search(r'```json\n(.*?)\n```', rewrite_result.content, re.DOTALL)
        json_str = json_match.group(1)
        search_query = json.loads(json_str)
    except Exception as e:
        print(f"Error parsing JSON: {e}")
        search_query = {'is_change': True, 'query_rewrite': query} # å…œåº•
    print(f"[simple_qa]rewrite search_query = {search_query['query_rewrite']}")

    ## step2. è¿›è¡Œæœç´¢æˆ–è€…ä»æœ¬åœ°æ•°æ®åº“ä¸­è·å–ä¿¡æ¯
    search_result_list = baidu_search(search_query['query_rewrite'],top_k=20)
    #search_result_listæ˜¯{ 'url': item['url'], 'title': item['title'], 'content': item['content'] }æ ¼å¼
    if len(search_result_list) == 0:
        print("[simple_qa]No search results found.[baidu_search]error!")
        return "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„æœç´¢ç»“æœï¼Œè¯·å°è¯•æ›´æ¢é—®é¢˜æˆ–é‡è¯•ã€‚"

    ## step3. è·å–æœ€ç›¸å…³æ–‡æ¡£
    if need_search and len(search_result_list) > max_search_num:  # å¦‚æœæœç´¢ç»“æœè¶…è¿‡10æ¡ï¼Œé€‰å‡ºtop 10æœ€ç›¸å…³çš„
        # åˆ†åˆ«è·å–rewrite_resultå’Œsearch_result_listçš„å‘é‡ï¼Œè°ƒç”¨get_text_embeddingå‡½æ•°
        rewrite_vector = get_text_embedding(search_query['query_rewrite'])
        search_vectors = [get_text_embedding(item['title']+' '+item['content']) for item in search_result_list]
        if rewrite_result is None or search_vectors is None:
            print("[simple_qa]Error: Failed to get embeddings for rewrite query or search results.")
            return "æ— æ³•è·å–ç›¸å…³çš„æœç´¢ç»“æœï¼Œè¯·ç¨åå†è¯•ã€‚"
        # search_vectorså˜æˆnumpyæ•°ç»„
        search_np_vectors = np.array(search_vectors) # Nä¸ª1024ç»´åº¦å‘é‡
        relevant_search_results = get_top_k_search_results(search_result_list, rewrite_vector, search_np_vectors, top_k=max_search_num)
    else:
        relevant_search_results = search_result_list

    print(f"[simple_qa]relevant_search_results\n {relevant_search_results}")

    ## step4 å¯»æ‰¾æœ€ç›¸å…³çš„2ä»½ç½‘é¡µï¼Œä»”ç»†é˜…è¯»å†…å®¹
    chunk = []
    for i in range(len(relevant_search_results)):
        doc = relevant_search_results[i]
        chunk.append(f"Document {i+1}ï¼š titleï¼š{doc['title']}ï¼Œcotentï¼š{doc['content']}\n")
    find_most_relevant_paper_prompt_filled = find_most_relevant_html_prompt_template_financial.format(
        query=query,
        search_results="\n".join(chunk)
    )
    # æœ€ç›¸å…³çš„2ä»½ç½‘é¡µï¼Œè¿›å…¥ç½‘é¡µä»”ç»†é˜…è¯»å†…å®¹ï¼Œå…¶ä»–çš„åªä½¿ç”¨snippet
    try: 
        most_relevant_html_result = deepseek_v3.invoke(find_most_relevant_paper_prompt_filled)
        print('[simple_qa]most_relevant_html_result = ', most_relevant_html_result.content)
        # è§£æè¿”å›çš„index
        most_relevant_indices = []
        if most_relevant_html_result.content:
            parts = most_relevant_html_result.content.split(',')
            for part in parts:
                part = part.strip()
                if part.isdigit():
                    idx = int(part)
                    if 0 <= idx < len(relevant_search_results):
                        most_relevant_indices.append(idx)
        print(f"[simple_qa]most_relevant_index = {most_relevant_indices}")
        most_relevant_html_list = [relevant_search_results[i] for i in most_relevant_indices]
        # ä¸‹è½½å¹¶ä¸”è¿›å…¥è¯¥ç½‘é¡µï¼Œç†è§£è¯¥ç½‘é¡µçš„å†…å®¹
        html_content_list = [] # å›ç­”çš„ç­”æ¡ˆ
        for i in range(len(most_relevant_html_list)):
            doc = most_relevant_html_list[i]
            visited = set() # è®°å½•å·²ç»è®¿é—®è¿‡çš„url
            status, sc_answer = await dfs_click_url_financial(query, doc['url'], doc['url'], 1, 2, visited)
            html_content_list.append(sc_answer)
    except Exception as e:
        print(f"[simple_qa]Error invoking model to find most relevant paper: {e}")
        return "æ— æ³•ç¡®å®šæœ€ç›¸å…³çš„ç½‘é¡µå†…å®¹ï¼Œè¯·ç¨åå†è¯•ã€‚"
    
    ## step5. å›ç­”é—®é¢˜
    chunk = []
    cnt = 1
    for i in range(len(relevant_search_results)):
        if i in most_relevant_indices and len(html_content_list)>0:
            continue
        doc = relevant_search_results[i]
        chunk.append(f"Index [{cnt}]ï¼š titleï¼š{doc['title']}ï¼Œcotentï¼š{doc['content']}\n")
        cnt += 1
    for i in range(len(html_content_list)):
        html_content = html_content_list[i]
        chunk.append(f"Index [{cnt}]ï¼š contentï¼š{html_content}\n")
        cnt += 1

    search_answer_prompt = search_answer_prompt_template_en.format(
        query=query,
        relevant_docs="\n".join(chunk)
    )
    try:
        llm_answer = deepseek_v3.invoke(search_answer_prompt)
    except Exception as e:
        print(f"[simple_qa]Error invoking model: {e}")
        return "å›ç­”é—®é¢˜æ—¶å‘ç”Ÿé”™è¯¯ï¼Œè¯·ç¨åå†è¯•ã€‚"
    print('[simple_qa]llm_answer = ', llm_answer.content)
    return llm_answer.content




# æ”¯æŒç®€å•çš„é—®ç­”ç³»ç»Ÿ
async def simple_qa_en_baidu(query,max_search_num=10):
    need_search = True
    print('[simple_qa]original query = ',query)
    search_rewrite_prompt = search_rewrite_template_en.format(
        date=datetime.now().strftime("%Y-%m-%d"),
        query=query
    )

    ##  step1. é—®é¢˜æ”¹å†™
    rewrite_result = deepseek_v3.invoke(search_rewrite_prompt)
    print('[simple_qa]rewrite query = ',rewrite_result.content)
    
    try:
        json_match = re.search(r'```json\n(.*?)\n```', rewrite_result.content, re.DOTALL)
        json_str = json_match.group(1)
        search_query = json.loads(json_str)
    except Exception as e:
        print(f"Error parsing JSON: {e}")
        search_query = {'is_change': True, 'query_rewrite': query} # å…œåº•
    print(f"[simple_qa]rewrite search_query = {search_query['query_rewrite']}")

    ## step2. è¿›è¡Œæœç´¢æˆ–è€…ä»æœ¬åœ°æ•°æ®åº“ä¸­è·å–ä¿¡æ¯
    search_result_list = baidu_search(search_query['query_rewrite'],top_k=20)
    #search_result_listæ˜¯{ 'url': item['url'], 'title': item['title'], 'content': item['content'] }æ ¼å¼
    if len(search_result_list) == 0:
        print("[simple_qa]No search results found.[baidu_search]error!")
        return "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„æœç´¢ç»“æœï¼Œè¯·å°è¯•æ›´æ¢é—®é¢˜æˆ–é‡è¯•ã€‚"

    ## step3. è·å–æœ€ç›¸å…³æ–‡æ¡£
    if need_search and len(search_result_list) > max_search_num:  # å¦‚æœæœç´¢ç»“æœè¶…è¿‡10æ¡ï¼Œé€‰å‡ºtop 10æœ€ç›¸å…³çš„
        # åˆ†åˆ«è·å–rewrite_resultå’Œsearch_result_listçš„å‘é‡ï¼Œè°ƒç”¨get_text_embeddingå‡½æ•°
        rewrite_vector = get_text_embedding(search_query['query_rewrite'])
        search_vectors = [get_text_embedding(item['title']+' '+item['content']) for item in search_result_list]
        if rewrite_result is None or search_vectors is None:
            print("[simple_qa]Error: Failed to get embeddings for rewrite query or search results.")
            return "æ— æ³•è·å–ç›¸å…³çš„æœç´¢ç»“æœï¼Œè¯·ç¨åå†è¯•ã€‚"
        # search_vectorså˜æˆnumpyæ•°ç»„
        search_np_vectors = np.array(search_vectors) # Nä¸ª1024ç»´åº¦å‘é‡
        relevant_search_results = get_top_k_search_results(search_result_list, rewrite_vector, search_np_vectors, top_k=max_search_num)
    else:
        relevant_search_results = search_result_list

    print(f"[simple_qa]relevant_search_results\n {relevant_search_results}")

    ## step4 å¯»æ‰¾æœ€ç›¸å…³çš„2ä»½ç½‘é¡µï¼Œä»”ç»†é˜…è¯»å†…å®¹
    chunk = []
    for i in range(len(relevant_search_results)):
        doc = relevant_search_results[i]
        chunk.append(f"Document {i+1}ï¼š titleï¼š{doc['title']}ï¼Œcotentï¼š{doc['content']}\n")
    find_most_relevant_paper_prompt_filled = find_most_relevant_html_prompt_template.format(
        query=query,
        search_results="\n".join(chunk)
    )
    # æœ€ç›¸å…³çš„2ä»½ç½‘é¡µï¼Œè¿›å…¥ç½‘é¡µä»”ç»†é˜…è¯»å†…å®¹ï¼Œå…¶ä»–çš„åªä½¿ç”¨snippet
    try: 
        most_relevant_html_result = deepseek_v3.invoke(find_most_relevant_paper_prompt_filled)
        print('[simple_qa]most_relevant_html_result = ', most_relevant_html_result.content)
        # è§£æè¿”å›çš„index
        most_relevant_indices = []
        if most_relevant_html_result.content:
            parts = most_relevant_html_result.content.split(',')
            for part in parts:
                part = part.strip()
                if part.isdigit():
                    idx = int(part)
                    if 0 <= idx < len(relevant_search_results):
                        most_relevant_indices.append(idx)
        print(f"[simple_qa]most_relevant_index = {most_relevant_indices}")
        most_relevant_html_list = [relevant_search_results[i] for i in most_relevant_indices]
        # ä¸‹è½½å¹¶ä¸”è¿›å…¥è¯¥ç½‘é¡µï¼Œç†è§£è¯¥ç½‘é¡µçš„å†…å®¹
        html_content_list = [] # å›ç­”çš„ç­”æ¡ˆ
        for i in range(len(most_relevant_html_list)):
            doc = most_relevant_html_list[i]
            visited = set() # è®°å½•å·²ç»è®¿é—®è¿‡çš„url
            status, sc_answer = await dfs_click_url(query, doc['url'], doc['url'], 1, 2, visited)
            html_content_list.append(sc_answer)
    except Exception as e:
        print(f"[simple_qa]Error invoking model to find most relevant paper: {e}")
        return "æ— æ³•ç¡®å®šæœ€ç›¸å…³çš„ç½‘é¡µå†…å®¹ï¼Œè¯·ç¨åå†è¯•ã€‚"
    
    ## step5. å›ç­”é—®é¢˜
    chunk = []
    cnt = 1
    for i in range(len(relevant_search_results)):
        if i in most_relevant_indices and len(html_content_list)>0:
            continue
        doc = relevant_search_results[i]
        chunk.append(f"Index [{cnt}]ï¼š titleï¼š{doc['title']}ï¼Œcotentï¼š{doc['content']}\n")
        cnt += 1
    for i in range(len(html_content_list)):
        html_content = html_content_list[i]
        chunk.append(f"Index [{cnt}]ï¼š contentï¼š{html_content}\n")
        cnt += 1

    search_answer_prompt = search_answer_prompt_template_en.format(
        query=query,
        relevant_docs="\n".join(chunk)
    )
    try:
        llm_answer = deepseek_v3.invoke(search_answer_prompt)
    except Exception as e:
        print(f"[simple_qa]Error invoking model: {e}")
        return "å›ç­”é—®é¢˜æ—¶å‘ç”Ÿé”™è¯¯ï¼Œè¯·ç¨åå†è¯•ã€‚"
    print('[simple_qa]llm_answer = ', llm_answer.content)
    return llm_answer.content



# æœç´¢é‡‘èä¿¡æ¯
async def financial_search(question):
    # step1 ä»»åŠ¡æ”¹å†™
    task_rewrite_prompt = task_rewrite_template_financial.format(date=datetime.now().strftime("%Y-%m-%d"), task=question)
    task_rewrite_response = deepseek_v3.invoke(task_rewrite_prompt)
    print('æ”¹å†™åçš„é—®é¢˜ï¼š\n', task_rewrite_response.content)

    rewrite_query = task_rewrite_response.content

    # step2 å­ä»»åŠ¡æ‹†åˆ†
    sub_task_divide_prompt = sub_task_divide_template_financial.format(task=rewrite_query)

    sub_task_divide_response = qwen_flash.invoke(sub_task_divide_prompt)
    print("å­ä»»åŠ¡æ‹†è§£ç»“æœï¼š\n", sub_task_divide_response.content)
    try:
        subtasks = generate_subtasks(sub_task_divide_response.content)
        print("å­ä»»åŠ¡åˆ—è¡¨ï¼š\n", subtasks)
    except Exception as e:
        print("å­ä»»åŠ¡ç”Ÿæˆå¤±è´¥ï¼š\n", e)
        # è¿™ç§æƒ…å†µä¸‹åŸå§‹ä»»åŠ¡å°†è¢«ä¿ç•™
        subtasks = [rewrite_query]
        if_success = 'no'
        
    # step3 å­ä»»åŠ¡å›ç­”
    subtask_answer_list = []
    need_search = 0
    for subtask in subtasks:
        question_router_template = question_router_template_financial.format(subtask=subtask)
        question_router_response = qwen_flash.invoke(question_router_template)
        print('å½“å‰å­é—®é¢˜ä¸ºï¼š\n', subtask)
        print('[é—®é¢˜è·¯ç”±]åˆ¤æ–­å½“å‰é—®é¢˜æ˜¯å¦éœ€è¦è°ƒç”¨æœç´¢å¼•æ“')
        if 'yes' in (question_router_response.content.strip()).lower():
            need_search = 1
            print(f"å­ä»»åŠ¡ '{subtask}' éœ€è¦è°ƒç”¨baiduæœç´¢å¼•æ“è·å–å®æ—¶ä¿¡æ¯ã€‚")
        else:
            need_search = 0
            print(f"å­ä»»åŠ¡ '{subtask}' å¯ä»¥ç›´æ¥å›ç­”ï¼Œæ— éœ€è°ƒç”¨æœç´¢å¼•æ“ã€‚")

        # åˆ¤æ–­æ¨¡å‹å†…éƒ¨çŸ¥è¯†æ˜¯å¦å¤Ÿç”¨ï¼Œä¸å¤Ÿè°ƒç”¨æœç´¢å¼•æ“
        if need_search==1:
            # è°ƒç”¨æœç´¢å¼•æ“è·å–å®æ—¶ä¿¡æ¯
            print('å¼€å§‹è°ƒç”¨baiduæœç´¢å¼•æ“è·å–å®æ—¶ä¿¡æ¯...')
            try:
                search_answer = await simple_qa_en_baidu_financial(subtask,max_search_num=10)
                print(f"å­ä»»åŠ¡ '{subtask}' çš„æœç´¢å¼•æ“å›ç­”ä¸ºï¼š\n", search_answer)
                subtask_answer_list.append(search_answer)
            except Exception as e:
                print(f"å­ä»»åŠ¡ '{subtask}' çš„æœç´¢å¼•æ“è°ƒç”¨å¤±è´¥ï¼š\n", e)
                subtask_answer_list.append("search_failed")
                if_success = 'no'
        else: # ç›´æ¥å›ç­”
            try:
                direct_prompt = direct_answer_template_financial.format(subtask=subtask)
                direct_answer = deepseek_v3.invoke(direct_prompt)
                print(f"å­ä»»åŠ¡ '{subtask}' çš„ç›´æ¥å›ç­”ä¸ºï¼š\n", direct_answer)
                subtask_answer_list.append(direct_answer.content)
            except Exception as e:
                print(f"å­ä»»åŠ¡ '{subtask}' çš„ç›´æ¥å›ç­”è°ƒç”¨å¤±è´¥ï¼š\n", e)
                subtask_answer_list.append("direct_answer_failed")
                if_success = 'no'

    # step4 æ±‡æ€»å›ç­”
    reference_list = []
    idx = 0
    for subtask in subtasks:
        ref_one = f"é—®é¢˜{idx+1}: {subtask}"
        ref_one += f"\nå›ç­”: {subtask_answer_list[idx].strip()}\n"
        reference_list.append(ref_one)
        idx += 1

    full_answer_generation_prompt = full_answer_generation_template_financial.format(
        task=question,
        references="\n".join(reference_list)
    )
    print("ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆçš„æç¤ºè¯­ï¼š\n", full_answer_generation_prompt)

    full_answer = qwen_flash.invoke(full_answer_generation_prompt)
    if full_answer:
        json_result = {}
        try:
            json_result = json.loads(full_answer.content.strip().replace("```json", "").replace("```", "").strip('`'))
        except Exception as e:
            print(f"Error parsing JSON: {e}")
            json_result = {"finished": "no", "content": full_answer.content}
    print(f"ç”Ÿæˆçš„æœ€ç»ˆç­”æ¡ˆï¼š\n{json_result['content']}")
    return json_result['content']


async def multi_step_qa_en(question):
    # step1 ä»»åŠ¡æ”¹å†™
    task_rewrite_prompt = task_rewrite_template_en.format(date=datetime.now().strftime("%Y-%m-%d"), task=question)
    task_rewrite_response = deepseek_v3.invoke(task_rewrite_prompt)
    print('æ”¹å†™åçš„é—®é¢˜ï¼š\n', task_rewrite_response.content)

    rewrite_query = task_rewrite_response.content

    # step2 å­ä»»åŠ¡æ‹†åˆ†
    sub_task_divide_prompt = sub_task_divide_template_en.format(task=rewrite_query)

    sub_task_divide_response = qwen_flash.invoke(sub_task_divide_prompt)
    print("å­ä»»åŠ¡æ‹†è§£ç»“æœï¼š\n", sub_task_divide_response.content)
    try:
        subtasks = generate_subtasks(sub_task_divide_response.content)
        print("å­ä»»åŠ¡åˆ—è¡¨ï¼š\n", subtasks)
    except Exception as e:
        print("å­ä»»åŠ¡ç”Ÿæˆå¤±è´¥ï¼š\n", e)
        # è¿™ç§æƒ…å†µä¸‹åŸå§‹ä»»åŠ¡å°†è¢«ä¿ç•™
        subtasks = [rewrite_query]
        if_success = 'no'
        
    # step3 å­ä»»åŠ¡å›ç­”
    subtask_answer_list = []
    need_search = 0
    for subtask in subtasks:
        question_router_template = question_router_template_en.format(subtask=subtask)
        question_router_response = qwen_flash.invoke(question_router_template)
        print('å½“å‰å­é—®é¢˜ä¸ºï¼š\n', subtask)
        print('[é—®é¢˜è·¯ç”±]åˆ¤æ–­å½“å‰é—®é¢˜æ˜¯å¦éœ€è¦è°ƒç”¨æœç´¢å¼•æ“')
        if 'search' in (question_router_response.content.strip()).lower():
            need_search = 1
            print(f"å­ä»»åŠ¡ '{subtask}' éœ€è¦è°ƒç”¨baiduæœç´¢å¼•æ“è·å–å®æ—¶ä¿¡æ¯ã€‚")
        elif 'arxiv' in (question_router_response.content.strip()).lower():
            need_search = 2
            print(f"å­ä»»åŠ¡ '{subtask}' éœ€è¦è°ƒç”¨arxivè·å–å­¦æœ¯ä¿¡æ¯ã€‚")
        else:
            need_search = 0
            print(f"å­ä»»åŠ¡ '{subtask}' å¯ä»¥ç›´æ¥å›ç­”ï¼Œæ— éœ€è°ƒç”¨æœç´¢å¼•æ“ã€‚")

        # åˆ¤æ–­æ¨¡å‹å†…éƒ¨çŸ¥è¯†æ˜¯å¦å¤Ÿç”¨ï¼Œä¸å¤Ÿè°ƒç”¨æœç´¢å¼•æ“
        if need_search==1:
            # è°ƒç”¨æœç´¢å¼•æ“è·å–å®æ—¶ä¿¡æ¯
            print('å¼€å§‹è°ƒç”¨baiduæœç´¢å¼•æ“è·å–å®æ—¶ä¿¡æ¯...')
            try:
                search_answer = await simple_qa_en_baidu(subtask,max_search_num=10)
                print(f"å­ä»»åŠ¡ '{subtask}' çš„æœç´¢å¼•æ“å›ç­”ä¸ºï¼š\n", search_answer)
                subtask_answer_list.append(search_answer)
            except Exception as e:
                print(f"å­ä»»åŠ¡ '{subtask}' çš„æœç´¢å¼•æ“è°ƒç”¨å¤±è´¥ï¼š\n", e)
                subtask_answer_list.append("search_failed")
                if_success = 'no'
        elif need_search==2:
            print('å¼€å§‹è°ƒç”¨arxivè·å–å­¦æœ¯ä¿¡æ¯...')
            # arXivæœç´¢éœ€è¦æ…¢ä¸€äº›
            sleep(1)
            try:
                # ä¸‹é¢æ˜¯åŒæ­¥å‡½æ•°
                loop = asyncio.get_event_loop()
                # ç›´æ¥è°ƒç”¨åŒæ­¥å‡½æ•°
                arxiv_answer = await loop.run_in_executor(None, simple_qa_en_arxiv, subtask,10)
                #arxiv_answer = simple_qa_en_arxiv(subtask,max_search_num=10)
                print(f"å­ä»»åŠ¡ '{subtask}' çš„arxivå›ç­”ä¸ºï¼š\n", arxiv_answer)
                subtask_answer_list.append(arxiv_answer)
            except Exception as e:
                print(f"å­ä»»åŠ¡ '{subtask}' çš„arxivè°ƒç”¨å¤±è´¥ï¼š\n", e)
                subtask_answer_list.append("arxiv_failed")
                if_success = 'no'

        else: # ç›´æ¥å›ç­”
            try:
                direct_prompt = direct_answer_template_en.format(subtask=subtask)
                direct_answer = deepseek_v3.invoke(direct_prompt)
                print(f"å­ä»»åŠ¡ '{subtask}' çš„ç›´æ¥å›ç­”ä¸ºï¼š\n", direct_answer)
                subtask_answer_list.append(direct_answer.content)
            except Exception as e:
                print(f"å­ä»»åŠ¡ '{subtask}' çš„ç›´æ¥å›ç­”è°ƒç”¨å¤±è´¥ï¼š\n", e)
                subtask_answer_list.append("direct_answer_failed")
                if_success = 'no'

    # step4 æ±‡æ€»å›ç­”
    reference_list = []
    idx = 0
    for subtask in subtasks:
        ref_one = f"é—®é¢˜{idx+1}: {subtask}"
        ref_one += f"\nå›ç­”: {subtask_answer_list[idx].strip()}\n"
        reference_list.append(ref_one)
        idx += 1

    full_answer_generation_prompt = full_answer_generation_template_en.format(
        task=question,
        references="\n".join(reference_list)
    )
    print("ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆçš„æç¤ºè¯­ï¼š\n", full_answer_generation_prompt)

    full_answer = qwen_flash.invoke(full_answer_generation_prompt)
    if full_answer:
        json_result = {}
        try:
            json_result = json.loads(full_answer.content.strip().replace("```json", "").replace("```", "").strip('`'))
        except Exception as e:
            print(f"Error parsing JSON: {e}")
            json_result = {"finished": "no", "content": full_answer.content}
    print(f"ç”Ÿæˆçš„æœ€ç»ˆç­”æ¡ˆï¼š\n{json_result['content']}")
    return json_result['content']



# åˆ¤æ–­å†…å®¹æ˜¯å¦å¯ä»¥ç›´æ¥å›ç­”ï¼Œè¿˜æ˜¯éœ€è¦å°è¯•ç‚¹å‡»é“¾æ¥æŸ¥çœ‹å†…å®¹çš„promptï¼ˆæœç´¢å¼•æ“å¯èƒ½å¯èƒ½æœåˆ°ä¹‹åtop10çš„é‡Œé¢ï¼Œå¯ä»¥é€‰æ‹©æœ€æœ‰å¯èƒ½top_kåŒ…å«ç›¸å…³ä¿¡æ¯çš„é“¾æ¥ï¼Œè¿›å…¥é“¾æ¥å»çœ‹ä¿¡æ¯ï¼‰
if __name__ == "__main__":
    async def main1():
        max_click_num = 2 # æœ€å¤šdfsçš„æ·±åº¦æ˜¯3å±‚
        user_query = "I want to know when is the submission deadline for CVPR 2025."
        refer_url = "https://cvpr.thecvf.com/Conferences/2025" # èµ·å§‹çš„é“¾æ¥

        # è·å–ç½‘é¡µæ–‡æœ¬å†…å®¹
        md_container, text_content = await get_content_from_url(refer_url,if_text_only=False)
        retrieved_text = ""

        # å¦‚æœç½‘é¡µå†…å®¹å·²ç»åŒ…å«ç­”æ¡ˆï¼Œå°±ä¸éœ€è¦ç‚¹å‡»é“¾æ¥äº†ï¼Œå¦åˆ™å°±éœ€è¦ç‚¹å‡»é“¾æ¥
        check_answer_prompt = full_answer_generation_template_en.format(
            task=user_query,
            references=str(text_content[:3000])
        )

        check_answer_response = qwen_flash.invoke(check_answer_prompt)
        print(check_answer_response.content)


        json_result = {}
        try:
            json_result = json.loads(check_answer_response.content.strip().replace("```json", "").replace("```", "").strip('`'))
            retrieved_text += json_result.get('content','')
        except Exception as e:
            print(f"Error parsing JSON: {e}")
            json_result = {"finished": "no", "content": check_answer_response.content}
            retrieved_text += check_answer_response.content
        print(json_result)

        most_relevant_url_list = await get_relevant_links(user_query,refer_url)

        final_answer = ""
        visited = set() # å…¨å±€å˜é‡ï¼Œå­˜å‚¨å·²ç»è®¿é—®è¿‡çš„urlï¼Œé¿å…é‡å¤è®¿é—®
        for item in most_relevant_url_list:
            status, answer = dfs_click_url(user_query, item['url'], refer_url, 1, max_click_num, visited)
            if status == 'yes':
                final_answer = answer
                break
            elif answer:
                retrieved_text += '\n' + str(answer)
            # çœ‹ä¸€ä¸‹answeré‡Œé¢æ˜¯å¦åŒ…å«æœ€ç»ˆç­”æ¡ˆï¼Œå¦‚æœåŒ…å«çš„è¯ï¼Œå°±ç›´æ¥è¿”å›ï¼Œä¸ç”¨å†è°ƒç”¨äº†ï¼Œå…ˆå°è¯•å›ç­”
            full_answer_generattion_prompt = full_answer_generation_template_en.format(
                task=user_query,
                references=retrieved_text
            )
            final_response = qwen_flash.invoke(full_answer_generattion_prompt)
            if final_response:
                print(final_response.content)
                json_result = {}
                try:
                    json_result = json.loads(final_response.content.strip().replace("```json", "").replace("```", "").strip('`'))
                except Exception as e:
                    print(f"Error parsing JSON: {e}")
                    json_result = {"finished": "no", "content": final_response.content}
                print(json_result)
                retrieved_text += '\n' + json_result.get('content','')
                if json_result.get('finished','no') == 'yes':
                    print("Final Answer:")
                    print(json_result.get('content',''))
                    final_answer = json_result.get('content','')
                    break
        print(final_answer)
    #asyncio.run(main1())


    # query = 'Latest research on web agent.'
    # output_response = simple_qa_en_arxiv(query)
    # print('\næœ€ç»ˆå›ç­”ï¼š\n',output_response)

    # # å®šä¹‰ä¸€ä¸ªå¼‚æ­¥çš„ä¸»å‡½æ•°
    # async def main2():
    #     query = 'Tell me about the latest research on web agent.'
    #     answer = await simple_qa_en_baidu(query, max_search_num=5)
    #     print('\næœ€ç»ˆå›ç­”ï¼š\n', answer)
    # asyncio.run(main2())
    
    # async def main3():
    #     question = 'Tell me the latest web agent research products such as DeepResearch provided by OpenAI, Grok and Tongyi.'
    #     answer = await multi_step_qa_en(question)
    #     print('\næœ€ç»ˆå›ç­”ï¼š\n', answer)
    # asyncio.run(main3())

    async def main4():
        question = 'ä»Šå¹´æš‘æœŸå¤–å–å¤§æˆ˜ä¹‹åï¼Œå¤–å–ä¸Šï¼Œé¥¿äº†ä¹ˆå¸‚åœºä»½é¢é«˜è¿˜æ˜¯ç¾å›¢å¸‚åœºä»½é¢é«˜ï¼Ÿ'
        answer = await multi_step_qa_en(question)
        print('\næœ€ç»ˆå›ç­”ï¼š\n', answer)
    asyncio.run(main4())

    
        
